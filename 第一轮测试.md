# 第一轮测试

​		第一轮测试共有6道题目（四道必做题和一道二选一题）以及一道特殊选做题（详见文件最后部分），其中第5、第6道题二选一进行作答。每道题目均有数个小问，一些需要你编程完成，一些需要你进行学习、推导、分析，并记录过程和答案，我们会在面试时进行提问。

​		我们是以兴趣为导向的团队，本测试不是你用来获得学分或水成绩的课程大作业，我们希望参与测试的人能够在测试中开拓眼界、提高自己，请明确你参加本测试的目的，也请**尊重本测试的每一个参与者**，**禁止一切形式的抄袭行为！**我们会积极将你的答案与互联网上的相关资源进行比对，一经发现，将取消你的测试资格。

​		记录时，我们推荐使用Typora + LaTeX的组合，提交你的Markdown文档或导出的PDF文档。当然，如果你认为时间有限，也可以提交Word文档，我们不会因此降低对你的评价，但希望你最终提交的结果清晰易辨，不至于带来交流的不便。

​		我们已经在题目中标出了对应出题人，若有相关疑问可以在群内直接@对应出题人，除特殊情况外，我们**不建议**进行私聊。



## 提交要求

​		将最终的压缩包提交至 [AI派2022招新第一轮测试提交链接](https://workspace.jianguoyun.com/inbox/collect/debbcd5f37be46b0a85d38510bcd68db/submit)，详细要求与截止时间见链接中说明。

​		对于你选择的题目，你需要提交对应的代码文件、说明文档、图片文件（若有）和其他文件（若有）。

​		注意，若你使用markdown文件作为说明文档，请注意将所用的图片文件路径更改为相对路径，若最终提交的显示效果有误，我们不会为你改正。从torchvision等机器学习库中加载的数据集文件不用提交（第4、5题）。但对于其他数据集文件（第1、3题），请将其一并提交，并注意你的调用路径同样应为相对路径。



## 分数说明

​		在所有题目中，共提供了7道选做小题，完成其中任意4道小题即可拿到选做部分的满分（假设你在每道小题上的评分均为满分）。

​		对于你没有完成的必做题目，你可以选择尽力完成其中部分内容或选择不完成（提交空白文件夹），我们会酌情给分。



## 1 逻辑回归

*出题人：李宇达*

简单来说，逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。相信经过一段时间学习的你，对它已经不再陌生。

由线性回归我们可以知道：一个LR模型的好坏由权重w来决定。那如何获得一个好的参数呢？从最开始的LR模型到现在，已经有了非常多的方法，在这里我们讨论三种不同的梯度下降方式。请学习相关知识，完成以下任务：

### 具体要求

- 加载数据集data1和data2（见文件夹“LR_dataset”），其中数据集的最后一列为标签。观察数据特征，根据相应特性对数据进行恰当的处理。
- 实现经典版本逻辑回归，调整学习率和迭代次数，分析梯度的收敛情况。在两个数据集上进行十折交叉验证，分析不同测试集划分方式对结果的影响，考虑是否有过拟合现象。
- 给逻辑回归添加正则项，实现贝叶斯版逻辑回归。观察梯度下降的路径，分析泛化系数的作用，以及此时学习率所扮演的角色。同样的分别在两个数据集上进行十折交叉验证。
- 实现随机梯度下降，调整偏置系数，同样的在两个数据集上进行十折交叉验证，观察随机梯度下降模型在两个数据集上的表现与前面两个模型的差异，并且分析其原因。
- （选做）尝试实现Adam算法，通过自适应对学习率进行动态优化。

### 一些提示

- 数据的处理十分重要，特殊的特征对迭代过程中的梯度下降有着较大的影响，同时数据增强也有可能达到意想不到的效果。

- 在模型效果不理想时可以尝试输出梯度，观察梯度下降方向以及梯度的收敛情况，结合数据情况分析原因。

- 如果有精力可以对训练过程进行一定的可视化。

- 对于十折交叉验证，我们允许你调用sklearn实现该操作，但也仅限于该操作

  

## 2 支持向量机

*出题人：周承禹（群内ID：奇迹）*

支持向量机是在分类任务上的一种方法，如图：

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/2809864/1657354127858-a475d889-c668-42b8-80ad-d3dd4f764718.jpeg)

直观上看，我们应该找两类样本间最“正”的划分超平面，就像图中蓝线所示，因为该超平面对训练样本的局部扰动“抗干扰性”最好，也就是说，这个划分超平面所产生的分类效果是最好的，对未见的例子的泛化能力也最强。

### 具体要求

- 给出数学推到什么是硬间隔的KKT条件
- 利用逻辑回归一题处理后的数据，实现SVM，并用SMO算法优化
- 给出介绍，什么是软间隔，松弛变量，和软间隔的KKT条件
- （选做）实现软间隔SVM，并且用SMO算法优化
- （选做）了解合页损失函数，并且解释该损失函数与SVM关系，然后实现它

### 一些提示

- svm博客，知乎上有很多介绍，单啃西瓜书很难懂

- 数学推导，和代码在网上有很多，但是我们希望你能够真正理解，禁止抄袭！！！

  

## 3 Spaceship Tantic

*出题人：王祯华*

故事发生在遥远的2912年。宇宙飞船泰坦尼克号携带者13000名乘客在前往半人马座的阿尔法星时，与隐藏在尘埃云中的时空异常相撞。与一千年前那艘同名巨轮不同的是，这艘宇宙飞船完好无损，但几乎一半的乘客被运送到了另一个维度！

救援人员向你提供了飞船电脑中的数据，他们请你通过乘客的仓位，乘客的睡眠情况等等信息来预测到底那些乘客遭遇了意外。这些信息无法面面俱到，但事实表明通过有限的特征也可以预测出十分准确的结果。

在本题中，有两个数据集：test.csv, train.csv。由于test.csv中并没有供你进行测试的标签信息，你需要进行在线上传并由平台对你的成绩进行评分。

### 具体要求

- 注册一个Kaggle账号，加入到[Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)竞赛中；
- 正确读取train.csv数据，认识、了解数据特性，掌握基础的数据处理的方法并对数据进行恰当的处理，简述你的处理方法；
- 选择你认为恰当的模型，简述理由；
- 对你选择的模型进行代码实现，并进行性能评估；
- 按照网站要求上传你的预测结果，并留存最好的成绩。
- （选做）参考、比对多个模型的效果，分析原因。

### 供你参考

- 韦斯·麦金尼著；徐敬译. 利用Python进行数据分析 原书第2版. 北京：机械工业出版社, 2018.07.
- [Learn Python, Data Viz, Pandas & More | Tutorials | Kaggle](https://www.kaggle.com/learn)
- [Code Review - Spaceship Titanic Kaggle](https://www.youtube.com/watch?v=ParD52xRNsQ)
- [Comprehensive data exploration with Python | Kaggle](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python#5.-Getting-hard-core)

### 一些提示

- 题目中给出的数据集有一些缺失项，解题前应引起注意；
- 数据集中存在着数值型和非数值型数据，请选择恰当的方法对他们分别进行处理；
- 如果觉得本题一时不好解决，可以先跳过，后面题目所使用的方法可能对解决本题有所帮助
- 可以对数据进行一定的可视化分析，观察数据间的规律；

注：本题严禁直接调用已经封装好的各类机器学习库（包括但不限于sklearn），但可以用NumPy等数学运算库和Pandas.



## 4 神经网络之全连接神经网络

*出题人：王怡彬、陈奥威*

神经网络是时下深度学习技术的基础，本题目旨在考察同学们对神经网络基本原理的掌握程度；  

本题要求：尝试自行推导出神经网络的反向传播过程，并实现一个全连接神经网络，使用梯度下降法在 MNIST数据集上进行分类任务。  这里指定损失函数为交叉熵。在代码的必要处添加注释。

通过学习神经网络有关知识，请完成以下任务：

### 具体要求

- 使用任意一种机器学习库来加载MNIST数据集（例如torchvision或sklearn等等），但请注意你的训练集：测试集划分比例应为6：1
- 推导反向传播公式，并做文档记录
- 实现隐藏层为3层的全连接神经网络（每层神经元个数不限）
- 设计并实现一种学习率的调整方法（例如以固定间隔成倍衰减初始学习率，余弦式衰减等等），观察其与固定学习率的区别，对比其效果，尝试分析
- 实现两种及以上的激活函数，并观察对比其效果，尝试分析
- 实现随机梯度下降和（Stochastic Gradient Descent）和小批量梯度下降（Mini-Batch Gradient Descent），对比二者效果，尝试分析
- 实现交叉熵损失函数
- 汇报你的训练过程与训练结果，如果有精力可以对训练过程进行一定的可视化，呈现在你的说明文档中
- （选做）实现两种及以上的自适应学习率方法，并观察对比其效果，尝试分析
- （选做）实现隐藏层为5层的全连接神经网络，观察对比其与3层隐藏层网络的效果，尝试分析
- （选做）尝试添加正则化手段，例如权重衰减、dropout、batchnorm等，并观察其效果，尝试分析

### 供你参考

- Neural Networks - Cornell CS4780 Lecture Notes: http://www.cs.cornell.edu/courses/cs4780/ 2018fa/lectures/lecturenote20.pdf 
- 周志华.机器学习[M].北京:清华大学出版社,2016:425 
- 矩阵求导术：https://zhuanlan.zhihu.com/p/24709748  

### 一些提示

反向传播的推导过程中需要用到多元函数微分的知识，矩阵形式的表达是对该过程的简略书写，可以让推导过程更加简洁明了。我们推荐你使用这样的语言来描述问题，但这也需要一点矩阵微分的知识，我们在上述参考中给出一份有关矩阵微分的参考资料，同学们可以自行查找对应微分公式，尝试给出神经网络反向传播算法流程，并尽可能将其实现。  

 注：除去加载和调用数据集的部分外，本题严禁直接调用已经封装好的各类机器学习库（包括但不限于sklearn），但可以用NumPy等数学运算库. 



## 5 神经网络之卷积神经网络

*出题人：陈奥威、王怡彬*

在完成了基础的全连接网络后，不妨来挑战一下实现强大的卷积神经网络吧。卷积神经网络是在全连接神经网络的基础上实现的，你可以积极调用搜索引擎来查阅相关资料或参考下方提供的相关资料，但我们依然禁止任何形式的抄袭。

### 具体要求

- 使用任意一种机器学习库来加载MNIST数据集（例如torchvision或sklearn等等）
- 简述AI中卷积神经网络中的卷积与数学中的卷积的区别（数学中的卷积只要求知道卷积的数学表达式以及计算方法）。
- 请仅使用numpy库实现卷积运算操作并设计卷积神经网络，网络结构可参考lenet、alexnet等，也可自行设计，我们对于网络结构选择带来的精度改变不做要求。
- 在MNIST手写数据集上进行测试，请提交详细的说明文档、代码注释以及最终的结果。
- （选做）实现池化层，并观察其效果，尝试分析

 注：除去加载数据集的部分外，本题严禁直接调用已经封装好的各类机器学习库（包括但不限于sklearn），但可以用NumPy等数学运算库. 

### 供你参考

* 卷积神经网络反向传播的推导：https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/

* 池化层反向传播的推导：https://zhuanlan.zhihu.com/p/258604402



## 6 Self-Attention

*出题人：陈奥威*

自从2017年的那篇《Attention is all you need》论文发布以来，无数的NLP，CV，TimeSeries等领域纷纷对这篇论文里面所提出的Self-Attention机制进行改进以及探索，可以说，这是继卷积神经网络以来最火的研究方向了。为了跟进时代的步伐，我们将深入这篇论文，复现出里面的注意力机制：

阅读这篇论文，并做如下思考：

（1）思考什么是Self-Attention，理解Self-Attention的数学表达，说明并尝试用python代码实现这个机制。（本小问不允许使用除numpy之外的库实现，即不能使用任何深度学习框架）

（2）理解encoer-decoder机制，查阅相关资料，尝试动手实现一个encoder类和一个decoder类，此处允许使用任何深度学习框架，并且极力建议使用pytorch实现。

（3）请随意展开脑洞，发挥你的想象能力，首先简单阐述Self-Attention机制的原理，并在你的理解上提出一些改进Self-Attention的方法，并解释为什么这么考虑。（该小问的改进方法请勿在网上照搬照抄，可以看看相关改进论文并提出你对该论文的理解，如果能复现出来且注释详细且能达到与原论文接近的效果可以本大题直接满分，但是本题鼓励大家展开脑洞，只要有合理且真实正确的解释，本小问一律满分）。

【注】在思考本题目时，需要给出完整的说明文档，代码尽量行行注释，谢谢大家。一二问不知道如何下手的话，可以只看第三问，毕竟基本上不需要code的嘛。



## 神秘领域 AI system（选做）

*出题人：逯润雨*

欢迎来到AI派招新AI system部分, 此招新面对**`了解深度学习 and 具有一定高性能经验 and（具有C++ or cuda基础）`的同学**，也欢迎**对上述三个要求中至少满足两个**的爱好者。

### 一些忠告

本题目面对有一定相应基础并且有志于从事该方向的同学，**非诚勿扰**，并不会因为人少就降低要求，如果没有人符合就不会招收此方向新成员。如果你感觉到非常困难进行不下去或者对题目有疑问，欢迎及时在群里或者私聊我（QQ1785435713）提问交流，或者你此前只有一些python经验，建议更换题目，来日方长可以以后研究。

如若选择完成该题目，其他题目可酌情减量完成（例如，完成卷积神经网络即可），想参与的同学请私聊群内管理员逯润雨同学（QQ1785435713）



​		

